<!doctype html>
<html lang="en">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">

    <title>Getting the Data | AIdentify</title>
    <link rel="icon" type="image/x-icon" href="static/img/favicon.png">
  </head>
  <body class="d-flex h-100 text-center text-bg-dark">

    <div class="cover-container d-flex w-100 h-100 p-3 mx-auto flex-column">
      <div class="container-fluid">
        <header class="d-flex flex-wrap justify-content-center py-3 mb-4 border-bottom">
          <a href="/" class="d-flex align-items-center mb-3 mb-md-0 me-md-auto text-dark text-decoration-none">
                <img src="static/img/aidentify-icon-only.png" class="bi me-2" width="35" height="35"><use xlink:href="#bootstrap"></use></img>
                <span class="fs-4">AIdentify
                </span>
          </a>

          <ul class="nav nav-pills">
            <li class="nav-item"><a href="/" class="nav-link px-2 link-secondary" aria-current="page">Home</a></li>
            <li class="nav-item"><a href="/features1" class="nav-link px-2 link-secondary">Overview</a></li>
            <li class="nav-item"><a href="/features2" class="nav-link active active-secundary px-2 link-secondary" style="background-color: #11c7c3d9 !important;">Data</a></li>
            <li class="nav-item"><a href="/modelselection" class="nav-link px-2 link-secondary">Model</a></li>
            <li class="nav-item"><a href="team.html" class="nav-link px-2 link-secondary">Team</a></li>
            </ul>

        </header>
        <div class="container-fluid pb-3">
          <div class="d-grid gap-3" style="grid-template-columns: 0.8fr 1fr;">
            <div >
              <div class="text-md-start"><h4>Getting the Data</h4>
                <p>
                  The basis of any good machine learning project is rock-solid data. As they say, <strong> “garbage in equals garbage out!” </strong>, so the first step in our process was making sure we were feeding our model with only the finest data! </p>

<p> Through our research process we were able to identify two high-quality datasets with labels which matched what we wanted our model to classify. These two datasets were <strong> FER+ and AffectNet-HQ </strong>. These two datasets are made openly available through the Kaggle platform, and we give credit to the authors who first introduced them on this page. </p>

<p> The <strong> FER+ (Face Expression Recognition Plus) dataset </strong> is an extension/correction of the original FER-2013 dataset, which contained approximately 30,000 images of faces that are grayscaled and have been uniformly sized to 48x48 pixels. The FER+ dataset essentially takes the original FER-2013 dataset and re-labels the images to classify them into one of eight emotions: neutral, happy, sad, surprised, fearful, angry, disgust and contempt. This relabeling was down by 10 crowd-sourced taggers, which makes the labeling process more robust as it doesn’t just depend on the evaluation of a single labeler (as was the case in the original 2013 version of the dataset). </p>

<p> The <strong> AffectNet-HQ dataset </strong> is an annotated database of facial expressions “in the wild” (which are essentially candid photos which are not staged just for the benefit of the dataset). The database was created by collecting facial images from the internet by querying three major search engines using a large number of emotion related keywords in six different languages. The version of the database available on Kaggle has been cleaned to relabel the emotional classes of certain images and utilizes an image enhancement technique to improve image quality. In total the dataset contains about 31,000 images of varying pixel sizes. </p>

<p> These two datasets together form the foundation of the information our AI model learned from. Continue to the next page to find out how we prepared this data to make it model-ready! </p>
                </p>
                </div>
            </div>
            <div>
                <div class="bg-light border rounded-3">
                  <img src="static/img/page_imgs/fer_emotions.png" class="img-fluid" alt="Responsive image">
                  <p> <u> Dataset credits: </u> </p>

                    <p> Barsoum, E., Zhang, C., Canton-Ferrer, C., & Zhang, Z. (2016). Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution. CoRR, abs/1608.01041. </p>

                    <p> Mollahosseini, A., Hassani, B., & Mahoor, M. H. (2017). AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild. CoRR, abs/1708.03985. </p>
                     </p>
                </div>
    <nav class="navbar fixed-bottom navbar-expand-sm ">
        <div class="container d-flex flex-wrap">
          <ul class="nav me-auto">
            <a href="/features1" class='btn btn-primary align-items-left' style="border-color:#11c7c3d9 !important; background-color: #11c7c3d9 !important;">&lt;&lt;Back</a>         </ul>
          </ul>
          <ul class="nav">
            <a href="/features3" class='btn btn-primary' style="border-color:#11c7c3d9 !important; background-color: #11c7c3d9 !important;">Continue>></a>
          </ul>
        </div>
    </nav>
  </div>
</div>
  </body>

</html>
